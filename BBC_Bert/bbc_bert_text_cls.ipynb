{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0921da32",
   "metadata": {},
   "source": [
    "# BBC Text Classification with BERT (Multi-class)\n",
    "\n",
    "This notebook adapts the workflow described in the article *Bert多分类&多标签文本分类实战* (run.py → data → model → train/eval/test) to **your dataset**: `/home/mywsl/Workspace/NLP/data/bbc_text_cls.csv`.\n",
    "\n",
    "- Dataset: BBC-style news, columns: `text`, `labels`\n",
    "- Task: **multi-class text classification**\n",
    "- Model: `bert-base-uncased` + Linear head\n",
    "\n",
    "> Tip: If you are running this notebook outside this sandbox, update `CSV_PATH` to your local path (e.g. `/home/mywsl/Workspace/NLP/data/bbc_text_cls.csv`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba8672",
   "metadata": {},
   "source": [
    "## 0) Install dependencies (run once)\n",
    "If you already have them, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1878c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch transformers==4.* scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26915a11",
   "metadata": {},
   "source": [
    "## 1) Imports & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f853035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mywsl/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import BertTokenizerFast, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdb59b",
   "metadata": {},
   "source": [
    "## 2) Config (mirrors the article's Config class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c60242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CSV_PATH': '/home/mywsl/Workspace/NLP/data/bbc_text_cls.csv',\n",
       " 'bert_name': 'bert-base-uncased',\n",
       " 'num_epochs': 3,\n",
       " 'batch_size': 32,\n",
       " 'max_length': 128,\n",
       " 'learning_rate': 5e-05,\n",
       " 'weight_decay': 0.01,\n",
       " 'require_improvement': 1000,\n",
       " 'output_dir': './outputs_bbc_bert',\n",
       " 'save_path': './outputs_bbc_bert/bert_bbc_cls.pt',\n",
       " 'device': device(type='cuda')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # --- data ---\n",
    "        self.CSV_PATH = os.getenv('CSV_PATH', '/home/mywsl/Workspace/NLP/data/bbc_text_cls.csv')\n",
    "\n",
    "        # --- model ---\n",
    "        # English dataset -> use an English pretrained model\n",
    "        self.bert_name = 'bert-base-uncased'\n",
    "\n",
    "        # --- training ---\n",
    "        self.num_epochs = 3\n",
    "        self.batch_size = 32\n",
    "        self.max_length = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.require_improvement = 1000  # early-stop patience in steps\n",
    "\n",
    "        # --- saving ---\n",
    "        self.output_dir = './outputs_bbc_bert'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.save_path = os.path.join(self.output_dir, 'bert_bbc_cls.pt')\n",
    "\n",
    "        # --- runtime ---\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "cfg.__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458fac1",
   "metadata": {},
   "source": [
    "## 3) Load CSV dataset\n",
    "Your file has multi-line quoted strings; `pandas.read_csv` handles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5138f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 2225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(cfg.CSV_PATH)\n",
    "\n",
    "# ensure expected columns\n",
    "df.columns = [c.strip().strip('\"') for c in df.columns]\n",
    "assert 'text' in df.columns and 'labels' in df.columns\n",
    "\n",
    "df = df.dropna(subset=['text', 'labels']).copy()\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['labels'] = df['labels'].astype(str)\n",
    "\n",
    "print('rows:', len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e849c6d",
   "metadata": {},
   "source": [
    "## 4) Encode labels & split train/dev/test\n",
    "We replicate the article's idea of separate train/dev/test sets, but generated from a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b560ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 5\n",
      "classes: ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
      "train/dev/test: 1780 222 223\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['labels'])\n",
    "class_list = list(le.classes_)\n",
    "num_classes = len(class_list)\n",
    "\n",
    "print('num_classes:', num_classes)\n",
    "print('classes:', class_list)\n",
    "\n",
    "train_df, tmp_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=1, stratify=df['label_id']\n",
    ")\n",
    "dev_df, test_df = train_test_split(\n",
    "    tmp_df, test_size=0.5, random_state=1, stratify=tmp_df['label_id']\n",
    ")\n",
    "\n",
    "print('train/dev/test:', len(train_df), len(dev_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66303955",
   "metadata": {},
   "source": [
    "## 5) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d27124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 190kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.86MB/s]\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.03MB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 1.97MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(cfg.bert_name)\n",
    "tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea0789",
   "metadata": {},
   "source": [
    "## 6) Dataset & DataLoader\n",
    "This replaces the article's `build_dataset()` and custom iterator, using PyTorch `Dataset/DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b289934c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([32, 128]),\n",
       " 'attention_mask': torch.Size([32, 128]),\n",
       " 'labels': torch.Size([32])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BBCDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: BertTokenizerFast, max_length: int):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = int(self.df.loc[idx, 'label_id'])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "train_loader = DataLoader(BBCDataset(train_df, tokenizer, cfg.max_length), batch_size=cfg.batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(BBCDataset(dev_df, tokenizer, cfg.max_length),   batch_size=cfg.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(BBCDataset(test_df, tokenizer, cfg.max_length),  batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "{k: v.shape for k, v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c079ff9",
   "metadata": {},
   "source": [
    "## 7) Model (BERT + Linear head)\n",
    "Matches the article's `Model` class: take pooled `[CLS]` representation and apply a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6caaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 420M/420M [00:22<00:00, 19.5MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForBBC(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertForBBC(nn.Module):\n",
    "    def __init__(self, bert_name: str, num_classes: int, finetune: bool = True):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = finetune\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None,\n",
    "            return_dict=False\n",
    "        )\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = BertForBBC(cfg.bert_name, num_classes, finetune=True).to(cfg.device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a022d0",
   "metadata": {},
   "source": [
    "## 8) Optimizer + Scheduler (AdamW + linear warmup)\n",
    "Same idea as the article: weight decay + warmup schedule. We use `torch.optim.AdamW` for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c51783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_optimizer_and_scheduler(model: nn.Module, cfg: Config, total_steps: int):\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    named_params = list(model.named_parameters())\n",
    "\n",
    "    grouped = [\n",
    "        {\n",
    "            'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': cfg.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in named_params if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(grouped, lr=cfg.learning_rate)\n",
    "\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "total_steps = len(train_loader) * cfg.num_epochs\n",
    "optimizer, scheduler = build_optimizer_and_scheduler(model, cfg, total_steps)\n",
    "(total_steps, int(total_steps*0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec9ec8",
   "metadata": {},
   "source": [
    "## 9) Evaluation (accuracy + report + confusion)\n",
    "Matches the article's `evaluate()` and `test()` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42adbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, data_loader: DataLoader, cfg: Config, class_list=None, return_report: bool = False):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(cfg.device)\n",
    "        attention_mask = batch['attention_mask'].to(cfg.device)\n",
    "        labels = batch['labels'].to(cfg.device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    acc = metrics.accuracy_score(all_labels, all_preds)\n",
    "    avg_loss = float(np.mean(losses))\n",
    "\n",
    "    if return_report:\n",
    "        report = metrics.classification_report(all_labels, all_preds, target_names=class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(all_labels, all_preds)\n",
    "        return acc, avg_loss, report, confusion\n",
    "\n",
    "    return acc, avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e2c76",
   "metadata": {},
   "source": [
    "## 10) Train loop (with early stopping + best checkpoint)\n",
    "This mirrors the article's train loop: evaluate on dev every N steps, save best, early-stop if no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca72451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/56 [00:05<04:57,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     0 | TrainLoss: 1.66 TrainAcc:31.25% | ValLoss: 1.73 ValAcc:16.67% | Time:5s *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 45/56 [02:47<01:00,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:   100 | TrainLoss: 0.04 TrainAcc:100.00% | ValLoss: 0.09 ValAcc:97.75% | Time:372s *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing best checkpoint ===\n",
      "Test Loss: 0.1078 | Test Acc: 97.3094%\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business     1.0000    0.9216    0.9592        51\n",
      "entertainment     1.0000    0.9487    0.9737        39\n",
      "     politics     0.9333    1.0000    0.9655        42\n",
      "        sport     1.0000    1.0000    1.0000        51\n",
      "         tech     0.9302    1.0000    0.9639        40\n",
      "\n",
      "     accuracy                         0.9731       223\n",
      "    macro avg     0.9727    0.9741    0.9724       223\n",
      " weighted avg     0.9749    0.9731    0.9731       223\n",
      "\n",
      "Confusion matrix:\n",
      "[[47  0  3  0  1]\n",
      " [ 0 37  0  0  2]\n",
      " [ 0  0 42  0  0]\n",
      " [ 0  0  0 51  0]\n",
      " [ 0  0  0  0 40]]\n"
     ]
    }
   ],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, dev_loader: DataLoader, test_loader: DataLoader, cfg: Config, class_list):\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_batch = 0\n",
    "    best_dev_loss = float('inf')\n",
    "    last_improve = 0\n",
    "    stop_flag = False\n",
    "\n",
    "    total_steps = len(train_loader) * cfg.num_epochs\n",
    "    optimizer, scheduler = build_optimizer_and_scheduler(model, cfg, total_steps)\n",
    "\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch [{epoch+1}/{cfg.num_epochs}]\")\n",
    "\n",
    "        for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "            input_ids = batch['input_ids'].to(cfg.device)\n",
    "            attention_mask = batch['attention_mask'].to(cfg.device)\n",
    "            labels = batch['labels'].to(cfg.device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if total_batch % 100 == 0:\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                train_acc = metrics.accuracy_score(labels.detach().cpu().numpy(), preds.detach().cpu().numpy())\n",
    "\n",
    "                dev_acc, dev_loss = evaluate(model, dev_loader, cfg)\n",
    "\n",
    "                improved = ''\n",
    "                if dev_loss < best_dev_loss:\n",
    "                    best_dev_loss = dev_loss\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'class_list': class_list,\n",
    "                        'label_encoder_classes': class_list,\n",
    "                        'bert_name': cfg.bert_name,\n",
    "                        'max_length': cfg.max_length,\n",
    "                    }, cfg.save_path)\n",
    "                    improved = '*'\n",
    "                    last_improve = total_batch\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                print(\n",
    "                    f\"Iter:{total_batch:>6} | \"\n",
    "                    f\"TrainLoss:{loss.item():>5.2f} TrainAcc:{train_acc:>6.2%} | \"\n",
    "                    f\"ValLoss:{dev_loss:>5.2f} ValAcc:{dev_acc:>6.2%} | \"\n",
    "                    f\"Time:{elapsed:>.0f}s {improved}\"\n",
    "                )\n",
    "                model.train()\n",
    "\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improve > cfg.require_improvement:\n",
    "                print('No optimization for a long time, auto-stopping...')\n",
    "                stop_flag = True\n",
    "                break\n",
    "\n",
    "        if stop_flag:\n",
    "            break\n",
    "\n",
    "    print('=== Testing best checkpoint ===')\n",
    "    ckpt = torch.load(cfg.save_path, map_location=cfg.device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "    test_acc, test_loss, report, confusion = evaluate(model, test_loader, cfg, class_list=class_list, return_report=True)\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4%}\")\n",
    "    print('Classification report:')\n",
    "    print(report)\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "\n",
    "\n",
    "train(model, train_loader, dev_loader, test_loader, cfg, class_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2913f5d",
   "metadata": {},
   "source": [
    "## 11) Single-text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93bd9d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'business', 0.9633949398994446)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_one(text: str, model: nn.Module, tokenizer: BertTokenizerFast, cfg: Config, class_list):\n",
    "    model.eval()\n",
    "\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        \n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=cfg.max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = enc['input_ids'].to(cfg.device)\n",
    "    attention_mask = enc['attention_mask'].to(cfg.device)\n",
    "\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    pred_id = int(probs.argmax())\n",
    "    return pred_id, class_list[pred_id], float(probs[pred_id]), probs\n",
    "\n",
    "\n",
    "text = 'Oil prices rise as traders fear supply disruptions'\n",
    "pred_id, pred_label, conf, _ = predict_one(text, model, tokenizer, cfg, class_list)\n",
    "(pred_id, pred_label, conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab540bb2",
   "metadata": {},
   "source": [
    "## 12) Notes for adapting to your local environment\n",
    "- If you run on your own machine, set `CSV_PATH` to `/home/mywsl/Workspace/NLP/data/bbc_text_cls.csv` or edit `cfg.CSV_PATH`.\n",
    "- For faster experiments, try `distilbert-base-uncased` (change `cfg.bert_name`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
